{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T21:27:43.250044Z",
     "start_time": "2019-05-29T21:27:42.252313Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/dev/Anaconda2-5.0.1/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import fastText as ft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T21:32:17.888112Z",
     "start_time": "2019-05-29T21:32:17.882659Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR='/temp0/dev/anjani/'\n",
    "train_file = BASE_DIR +'us_train_trn02525_10763169889_ascii_train.dat'\n",
    "train_file_out = BASE_DIR +'us_train_trn02525_10763169889_ascii_train_list.dat'\n",
    "val_file = BASE_DIR +'us_train_trn02525_10763169889_ascii_val.dat'\n",
    "test_file = BASE_DIR +'us_train_trn02525_10763169889_ascii_test.dat'\n",
    "fasttext_pretrain_model ='/temp0/dev/anjani/data/fasttext/tec_item_search/wiki.en.bin'\n",
    "filename = BASE_DIR+train_file\n",
    "\n",
    "class_list=[1070238745,1070238723,1070238752,1070240929,1070238731,1070238732,1070238728,1070253159,1070238750,1070238721,1070238749,1070238746,1070238733,1070238736,1070238735,1070238718,1070238716,1070253162,1070238726,1070238725,1070238748,1070238715,1070238739,1070238741,1070238742,1070238730,1070238719,1070238722,1070238729,1070238740,1070238751,1070238720,1070238743,1070238717,1070277868,1070253161,1070238724,1070238727,1070238738,1070238744,1070253160,1070238734,1070238737,1070240898,1070238747]\n",
    "class_size = len(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T21:32:20.716531Z",
     "start_time": "2019-05-29T21:32:20.713801Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_dict = dict(zip(class_list,range(len(class_list))))\n",
    "class_dict_rev = dict(zip(range(len(class_list)),class_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T21:32:40.505515Z",
     "start_time": "2019-05-29T21:32:21.465274Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ft_w2v_model = ft.load_model(fasttext_pretrain_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T12:53:18.754857Z",
     "start_time": "2019-05-25T12:53:18.751072Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T21:32:40.552675Z",
     "start_time": "2019-05-29T21:32:40.525622Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_size=len(class_list)\n",
    "def get_line_w2v(text):\n",
    "    return np.array(ft_w2v_model.get_sentence_vector(text)).astype('float32')  \n",
    "def preprocess_text(line):\n",
    "    line = str.upper(str(line))\n",
    "    return re.sub('[\\[\\]\\{\\}\\/,\\$\\-_:;\\'\\\"&\\(\\)#]',' ',line)\n",
    "\n",
    "\n",
    "def _parse_data(line):\n",
    "    line = tf.py_func(lambda x: preprocess_text(x),[line],tf.string)\n",
    "    line_split = tf.string_split(source=[line],delimiter=\"|\", skip_empty=False)\n",
    "    dept_name =  tf.py_func(lambda x: get_line_w2v(x),[line_split.values[1]],tf.float32)\n",
    "    class_name =  tf.py_func(lambda x: get_line_w2v(x),[line_split.values[2]],tf.float32)\n",
    "    brand_name =  tf.py_func(lambda x: get_line_w2v(x),[line_split.values[3]],tf.float32)\n",
    "    desc =  tf.py_func(lambda x: get_line_w2v(x),[line_split.values[4]],tf.float32)\n",
    "    label = tf.py_func(lambda x: class_dict[int(x)],[line_split.values[5]],tf.int64)\n",
    "    return tf.reshape(tf.stack([dept_name,class_name,brand_name,desc], axis=0),[-1]),\\\n",
    "tf.one_hot(label,depth=output_size)\n",
    "\n",
    "\n",
    "def create_features(line):\n",
    "    line = preprocess_text(line)\n",
    "    record = line.rstrip().split('|')\n",
    "    dept_name =get_line_w2v(record[1])\n",
    "    class_name =get_line_w2v(record[2])\n",
    "    brand_name =get_line_w2v(record[3])\n",
    "    desc =get_line_w2v(record[4])\n",
    "    label = np.eye(class_size)[class_dict[int(record[5])]].astype(np.int64)\n",
    "    features = np.reshape(np.hstack([dept_name,class_name,brand_name,desc]),1200).astype(np.float32)\n",
    "    return features,label\n",
    "\n",
    "def read_csv1(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            yield create_features(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T03:41:43.028146Z",
     "start_time": "2019-05-30T02:04:49.237801Z"
    }
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5c8f89a42794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbuffer_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer_cnt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer_cnt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mwr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mbuffer_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mbuffer_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "buffer_size=100\n",
    "buffer_cnt=0\n",
    "buffer_str = \"\"\n",
    "gen = read_csv1(train_file)\n",
    "with open(train_file_out, 'w+') as wr:\n",
    "    for rec in gen:\n",
    "        buffer_str = buffer_str + repr(list(rec[0]) + list(rec[1])) +\"\\n\"\n",
    "        buffer_cnt = buffer_cnt + 1\n",
    "        if buffer_cnt == buffer_size:\n",
    "            wr.write(buffer_str)\n",
    "            buffer_cnt = 0\n",
    "            buffer_str = \"\"\n",
    "    wr.write(buffer_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T12:54:22.839358Z",
     "start_time": "2019-05-25T12:54:22.778571Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim=1200\n",
    "num_dense1 = 300\n",
    "dropout1 = 0.2\n",
    "num_dense2 = 300\n",
    "dropout2 = 0.2\n",
    "STAMP = 'cnn_dense_%d_%.2f_%d_%.2f'%(num_dense1, dropout1,num_dense2, dropout2)\n",
    "bst_model_path = BASE_DIR + '/' +STAMP + '.h5'\n",
    "\n",
    "\n",
    "\n",
    "_input = tf.keras.Input(shape =(input_dim,), name = 'input')\n",
    "\n",
    "\n",
    "_input_reshape = tf.keras.layers.Reshape((input_dim,1,), name='Reshape')(_input)\n",
    "cnn_input_layer_1 = tf.keras.layers.Conv1D(filters=5, kernel_size=(3), strides=1, \\\n",
    "                                             padding='valid',activation='relu', name = 'cnn_input_layer_1')(_input_reshape)\n",
    "pool_input_layer_1 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=None, padding='valid',name ='pool_input_layer_1')(cnn_input_layer_1)#\n",
    "\n",
    "#cnn_input_layer_2 = tf.keras.layers.Conv1D(filters=5, kernel_size=(3), strides=1, \\\n",
    " #                                            padding='valid',activation='relu', name = 'cnn_input_layer_2')(pool_input_layer_1)\n",
    "#pool_input_layer_2 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=None, padding='valid',name ='pool_input_layer_2')(cnn_input_layer_2)\n",
    "\n",
    "\n",
    "x1 = tf.keras.layers.Flatten()(pool_input_layer_1)\n",
    "\n",
    "\n",
    "\n",
    "_dense_1 = tf.keras.layers.Dense(num_dense1, activation='relu', name = 'dense_layer_1')(x1)\n",
    "_dense_1 = tf.keras.layers.Dropout(dropout1)(_dense_1)\n",
    "\n",
    "#_dense_2 = tf.keras.layers.Dense(num_dense2, activation='relu', name = 'dense_layer_2')(_dense_1)\n",
    "#_dense_2 = tf.keras.layers.Dropout(dropout2)(_dense_2)\n",
    "\n",
    "_output = tf.keras.layers.Dense(output_size, activation='softmax', name='softmax')(_dense_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T12:54:28.227540Z",
     "start_time": "2019-05-25T12:54:28.198944Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(inputs=[_input], outputs=[_output])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T12:54:28.605173Z",
     "start_time": "2019-05-25T12:54:28.600629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "early_stopping =tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n",
    "ReduceLROnPlateau= tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "tbCallBack = tf.keras.callbacks.TensorBoard(log_dir=BASE_DIR, histogram_freq=1, write_graph=True, \\\n",
    "                                            write_images=True,write_grads=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T12:54:29.183466Z",
     "start_time": "2019-05-25T12:54:29.108975Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "d_train_file = tf.data.TextLineDataset(train_file).map(_parse_data)\n",
    "d_val_file = tf.data.TextLineDataset(val_file).map(_parse_data)\n",
    "d_test_file = tf.data.TextLineDataset(test_file).map(_parse_data)\n",
    "\n",
    "batched_train_file = d_train_file.batch(512)\n",
    "batched_val_file = d_val_file.batch(512)\n",
    "batched_test_file = d_test_file.batch(512)\n",
    "\n",
    "train_iterator = batched_train_file.make_one_shot_iterator()\n",
    "val_iterator = batched_val_file.make_one_shot_iterator()\n",
    "test_iterator = batched_test_file.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T04:39:12.867133Z",
     "start_time": "2019-05-25T12:54:29.835796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 2926s 293ms/step - loss: 0.2278 - acc: 0.9439 - val_loss: 0.1281 - val_acc: 0.9672\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 2934s 293ms/step - loss: 0.1385 - acc: 0.9650 - val_loss: 0.1145 - val_acc: 0.9715\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 2932s 293ms/step - loss: 0.1264 - acc: 0.9679 - val_loss: 0.1066 - val_acc: 0.9717\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 2945s 294ms/step - loss: 0.1196 - acc: 0.9693 - val_loss: 0.1002 - val_acc: 0.9739\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 2930s 293ms/step - loss: 0.1148 - acc: 0.9705 - val_loss: 0.0991 - val_acc: 0.9744\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 2914s 291ms/step - loss: 0.1113 - acc: 0.9713 - val_loss: 0.0935 - val_acc: 0.9753\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 2928s 293ms/step - loss: 0.1085 - acc: 0.9718 - val_loss: 0.0969 - val_acc: 0.9750\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 2937s 294ms/step - loss: 0.1066 - acc: 0.9723 - val_loss: 0.0874 - val_acc: 0.9761\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 2928s 293ms/step - loss: 0.1048 - acc: 0.9727 - val_loss: 0.0890 - val_acc: 0.9767\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 2931s 293ms/step - loss: 0.1031 - acc: 0.9730 - val_loss: 0.0879 - val_acc: 0.9766\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 2929s 293ms/step - loss: 0.1015 - acc: 0.9733 - val_loss: 0.0915 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.000200000009499.\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 2928s 293ms/step - loss: 0.0933 - acc: 0.9756 - val_loss: 0.0827 - val_acc: 0.9778\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 2869s 287ms/step - loss: 0.0911 - acc: 0.9761 - val_loss: 0.0795 - val_acc: 0.9782\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 2838s 284ms/step - loss: 0.0901 - acc: 0.9763 - val_loss: 0.0796 - val_acc: 0.9784\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 2848s 285ms/step - loss: 0.0898 - acc: 0.9763 - val_loss: 0.0773 - val_acc: 0.9799\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 2853s 285ms/step - loss: 0.0885 - acc: 0.9766 - val_loss: 0.0777 - val_acc: 0.9792\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 2861s 286ms/step - loss: 0.0886 - acc: 0.9767 - val_loss: 0.0781 - val_acc: 0.9801\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 2861s 286ms/step - loss: 0.0879 - acc: 0.9768 - val_loss: 0.0800 - val_acc: 0.9789\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.00000009499e-05.\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 2856s 286ms/step - loss: 0.0870 - acc: 0.9771 - val_loss: 0.0805 - val_acc: 0.9788\n",
      "Epoch 20/100\n",
      " 5313/10000 [==============>...............] - ETA: 21:52 - loss: 0.0871 - acc: 0.9770WARNING:tensorflow:Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5313/10000 [==============>...............] - ETA: 22:14 - loss: 0.0871 - acc: 0.9770 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 21/100\n",
      "WARNING:tensorflow:Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0/10000 [..............................] - ETA: 0s - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 2.00000013137e-06.\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_iterator, epochs=100, steps_per_epoch=10000,validation_data=val_iterator, validation_steps=100,  callbacks=[early_stopping, model_checkpoint,ReduceLROnPlateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:06:47.463548Z",
     "start_time": "2019-05-26T22:05:35.052062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 72s 241ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07534120067954063, 0.9797395833333333]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_iterator, steps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:08:11.917301Z",
     "start_time": "2019-05-26T22:08:11.913612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
